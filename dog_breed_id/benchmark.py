# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_benchmark.ipynb.

# %% auto 0
__all__ = ['TRAIN_TRANSFORMS', 'VALID_TRANSFORMS', 'iw', 'DataLoaders', 'get_id_mappings', 'get_class_weights',
           'get_data_subsets', 'DogBreedClassificationDataset', 'freeze_weights', 'init_model', 'RegularizerCB',
           'get_classification_accuracy', 'get_classification_accuracy_ensembled']

# %% ../nbs/04_benchmark.ipynb 4
from .data_preprocessing import read_csv_with_array_columns
from .research import get_classes_from_frame
from miniai.learner import *
from miniai.init import *
from miniai.activations import *
from miniai.datasets import show_images

import cv2
import fastcore.all as fc
from pathlib import Path
from PIL import Image
import pandas as pd
import numpy as np
import glob
import os
from matplotlib import pyplot as plt

import shutil
import torch
import torchvision
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import functional as F
from torcheval.metrics import MulticlassAccuracy
from sklearn.model_selection import train_test_split
import timm

# %% ../nbs/04_benchmark.ipynb 6
def get_id_mappings(df, include_background=True):
    classes = get_classes_from_frame(df, include_background=include_background)
    label2id = {label: i for i, label in enumerate(classes)}
    id2label = {i: label for i, label in enumerate(classes)}
    return label2id, id2label

def get_class_weights(df, label2id):
    weights = 1./df['category'].value_counts(normalize=True)
    weights = weights/weights.sum()
    keys = [label2id[l] for l in weights.index]
    order = np.argsort(keys)
    return torch.tensor(weights.values[order])

# %% ../nbs/04_benchmark.ipynb 9
def get_data_subsets(df, cache_path='../data', recreate=False):
    Path(cache_path).mkdir(parents=True, exist_ok=True)
    if not Path('../data/train.csv').exists() or recreate:
        train_subset, valid_subset = train_test_split(df, train_size=0.1, test_size=0.01, stratify=df['category'])
        valid_subset, test_subset = train_test_split(valid_subset, train_size=0.5, test_size=0.5)
        train_subset.to_csv('../data/train.csv')
        valid_subset.to_csv('../data/valid.csv')
        test_subset.to_csv('../data/test.csv')
    else:
        print('Cached files found, reading from disk')
        train_subset = read_csv_with_array_columns('../data/train.csv', ['bboxes'])
        valid_subset = read_csv_with_array_columns('../data/valid.csv', ['bboxes'])
        test_subset = read_csv_with_array_columns('../data/test.csv', ['bboxes'])
    return train_subset, valid_subset, test_subset

# %% ../nbs/04_benchmark.ipynb 11
class DogBreedClassificationDataset(Dataset):
    def __init__(self, df, label2id, id2label, transforms=None):
        super(DogBreedClassificationDataset, self).__init__()
        self.df = df
        if not transforms:
            self.transforms = torchvision.transforms.Compose([
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transforms = transforms
        self.label2id, self.id2label = label2id, id2label
    
    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, idx):
        item = self.df.iloc[idx]
        img = Image.open(item.image)
        tensor = self.transforms(img)
        label = self.label2id[item['category']]
        return tensor, label

TRAIN_TRANSFORMS = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ColorJitter(0.5, 0.5, 0.5),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.RandomErasing(p=0.1),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
VALID_TRANSFORMS = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# %% ../nbs/04_benchmark.ipynb 13
from functools import partial
iw = partial(init_weights, leaky=0.1)
def freeze_weights(p):
    p.requires_grad_(False)

def init_model(model, initialize=False, freeze_backbone=False):
    if initialize:
        model.apply(iw)
    if freeze_backbone:
        model.apply(freeze_weights)
    if hasattr(model, 'fc'):
        model.fc.requires_grad_(True)
    elif hasattr(model, 'head') and hasattr(model.head, 'fc'):
        model.head.requires_grad_(True)

# %% ../nbs/04_benchmark.ipynb 15
from collections import namedtuple

DataLoaders = namedtuple('DataLoaders', ['train', 'valid'])

# %% ../nbs/04_benchmark.ipynb 17
class RegularizerCB(Callback):
    def __init__(self, alpha=0.01): fc.store_attr()
    
    def after_loss(self, learn):
        param_sum = sum([(p**2).sum() for p in learn.model.parameters()])
        param_sum *= self.alpha
        learn.loss += param_sum

# %% ../nbs/04_benchmark.ipynb 38
def get_classification_accuracy(model, dl):
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    matches = []
    model.eval()
    model.to(device)
    for (imgs, labels) in dl:
        imgs = imgs.to(device)
        labels = labels.to(device)
        with torch.no_grad():
            logits = model(imgs)
        preds = logits.argmax(-1)
        matches.extend((preds == labels).cpu().tolist())
    return np.mean(matches)


def get_classification_accuracy_ensembled(models, dl):
    models = models if isinstance(models, list) else [models]
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    matches = []
    [model.eval() for model in models]
    models = [model.to(device) for model in models]
    for (imgs, labels) in dl:
        imgs = imgs.to(device)
        labels = labels.to(device)
        with torch.no_grad():
            # Do bagging
            logits = sum([model(imgs) for model in models])
            logits = logits/float(len(models))
        preds = logits.argmax(-1)
        matches.extend((preds == labels).cpu().tolist())
    return np.mean(matches)
