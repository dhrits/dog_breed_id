{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40634042-cc6b-4333-814c-99220a327678",
   "metadata": {},
   "source": [
    "# research\n",
    "\n",
    "> This notebook contains code for exploring existing research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e6eca0-99b1-4cce-83d2-2927a6d916d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f995f03-059b-4b70-8e65-f887065aef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab48f2-9c37-4033-9dc1-d0792e61b474",
   "metadata": {},
   "source": [
    "# Survey of Existing Research\n",
    "This notebook contains a survey of existing research of methods to do object detection and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a78e2a-8db9-4882-9be8-5fcf4ce5da39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173fff86-dd71-491a-9182-e724ac389e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os;print(os.environ['PYTORCH_ENABLE_MPS_FALLBACK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210ad57-2434-4898-8124-fdcd8a2b67db",
   "metadata": {},
   "source": [
    "## Building a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b3402a-865d-4ec5-94a2-b03960efd419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/dhritimansagar/Dev/dog_breed_id\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: dog_breed_id\n",
      "  Attempting uninstall: dog_breed_id\n",
      "    Found existing installation: dog_breed_id 0.0.1\n",
      "    Uninstalling dog_breed_id-0.0.1:\n",
      "      Successfully uninstalled dog_breed_id-0.0.1\n",
      "  Running setup.py develop for dog_breed_id\n",
      "Successfully installed dog_breed_id-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89f7502-35ef-4ded-a1f2-d3f27c5e7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dog_breed_id.data_preprocessing import read_csv_with_array_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af65f75-1648-43f0-9962-e655e4f6af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from miniai.datasets import show_images\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115dca96-05a3-4a95-b8a5-3b361b0c6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv_with_array_columns('../data/dogs.csv', ['bboxes'])\n",
    "subset = df.sample(frac=0.1)\n",
    "\n",
    "def copy_files(path):\n",
    "    target_path = str(path).replace('data', 'datasubset')\n",
    "    folder = Path(target_path).parent\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "    shutil.copy(path, target_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13094ec2-773d-48d1-a017-38786b678737",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('../datasubset').exists():\n",
    "    subset['image'].apply(copy_files)\n",
    "    subset.to_csv('../datasubset/dogs.csv')\n",
    "else:\n",
    "    subset = read_csv_with_array_columns('../datasubset/dogs.csv', ['bboxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f2439-6df7-46c8-b3e6-495e16f7fd20",
   "metadata": {},
   "source": [
    "## Mask RCNN\n",
    "Mask RCNN is from the RCNN family of object detectors. Add more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee44532-2d91-475d-8da7-a6d7cc9eca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67697de-c581-46e4-a1a7-d53f40b83afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_classes_from_frame(df, column=None):\n",
    "    \"\"\"gets the ['background'] + the rest of classes from a dataframe `df` with classes in column specified by `column`\"\"\"\n",
    "    if column is None: column = 'category'\n",
    "    classes = ['background'] + subset['category'].unique().tolist()\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a715ca-a5a5-49a5-831d-b75a6cd0e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DogsSubsetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.classes = get_classes_from_frame(df)\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(self.classes)\n",
    "\n",
    "    def _label2id(self, labels):\n",
    "        return self.le.transform(labels)\n",
    "\n",
    "    def _id2labels(self, ids):\n",
    "        return self.le.inverse_transform(labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        img = F.convert_image_dtype(read_image(item['image']), torch.float)\n",
    "        boxes = torch.as_tensor(item['bboxes'], dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        num_objs = boxes.shape[0]\n",
    "        labels  = torch.ones((num_objs,), dtype=torch.int64) * self._label2id([item['category']])[0]\n",
    "        image_id = idx\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def collate_fn(data):\n",
    "    images = [item[0] for item in data]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    targets = [item[1] for item in data]\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e9773e-2d10-4292-bc46-6b888fd67423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def get_fasterrcnn_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed9cea-a848-470f-815c-ccb7fe285683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75564c62-274c-4b25-bf1d-e0c6c51f10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from dog_breed_id import engine\n",
    "from dog_breed_id.engine import train_one_epoch, evaluate\n",
    "\n",
    "def train_faster_rcnn(df):\n",
    "        # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    #device = 'cpu'\n",
    "    print('Using device ', device)\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = len(get_classes_from_frame(df))\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = DogsSubsetDataset(df)\n",
    "    dataset_test = DogsSubsetDataset(df)\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=True,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=16, shuffle=False,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_fasterrcnn_model(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 2\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "    torch.save(model.state_dict(), 'model-fasterrcnn.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca04df2-dd51-4ad7-b2a3-ef4666efd3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/566]  eta: 1:48:49  lr: 0.000014  loss: 5.1205 (5.1205)  loss_classifier: 4.9846 (4.9846)  loss_box_reg: 0.1288 (0.1288)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0070 (0.0070)  time: 11.5367  data: 0.0242\n",
      "Epoch: [0]  [ 10/566]  eta: 1:13:12  lr: 0.000102  loss: 5.0822 (4.9635)  loss_classifier: 4.9337 (4.8175)  loss_box_reg: 0.1431 (0.1390)  loss_objectness: 0.0002 (0.0018)  loss_rpn_box_reg: 0.0050 (0.0052)  time: 7.9006  data: 0.0269\n",
      "Epoch: [0]  [ 20/566]  eta: 1:11:03  lr: 0.000191  loss: 4.2258 (3.9367)  loss_classifier: 4.0823 (3.7909)  loss_box_reg: 0.1431 (0.1387)  loss_objectness: 0.0002 (0.0023)  loss_rpn_box_reg: 0.0043 (0.0049)  time: 7.6228  data: 0.0271\n",
      "Epoch: [0]  [ 30/566]  eta: 1:09:55  lr: 0.000279  loss: 0.6968 (2.8527)  loss_classifier: 0.5480 (2.7024)  loss_box_reg: 0.1451 (0.1428)  loss_objectness: 0.0005 (0.0025)  loss_rpn_box_reg: 0.0045 (0.0050)  time: 7.7871  data: 0.0282\n",
      "Epoch: [0]  [ 40/566]  eta: 1:08:24  lr: 0.000367  loss: 0.5988 (2.3093)  loss_classifier: 0.4276 (2.1556)  loss_box_reg: 0.1497 (0.1464)  loss_objectness: 0.0006 (0.0024)  loss_rpn_box_reg: 0.0048 (0.0049)  time: 7.7966  data: 0.0301\n",
      "Epoch: [0]  [ 50/566]  eta: 1:07:20  lr: 0.000456  loss: 0.4974 (1.9389)  loss_classifier: 0.3590 (1.7876)  loss_box_reg: 0.1419 (0.1442)  loss_objectness: 0.0005 (0.0021)  loss_rpn_box_reg: 0.0050 (0.0050)  time: 7.8373  data: 0.0314\n",
      "Epoch: [0]  [ 60/566]  eta: 1:06:30  lr: 0.000544  loss: 0.3716 (1.6795)  loss_classifier: 0.2310 (1.5301)  loss_box_reg: 0.1358 (0.1421)  loss_objectness: 0.0010 (0.0022)  loss_rpn_box_reg: 0.0054 (0.0051)  time: 8.0586  data: 0.0322\n",
      "Epoch: [0]  [ 70/566]  eta: 1:06:06  lr: 0.000633  loss: 0.3458 (1.4914)  loss_classifier: 0.2071 (1.3437)  loss_box_reg: 0.1264 (0.1402)  loss_objectness: 0.0017 (0.0022)  loss_rpn_box_reg: 0.0057 (0.0053)  time: 8.4169  data: 0.0325\n",
      "Epoch: [0]  [ 80/566]  eta: 1:05:00  lr: 0.000721  loss: 0.3389 (1.3496)  loss_classifier: 0.2042 (1.2031)  loss_box_reg: 0.1284 (0.1391)  loss_objectness: 0.0013 (0.0021)  loss_rpn_box_reg: 0.0056 (0.0053)  time: 8.4477  data: 0.0345\n",
      "Epoch: [0]  [ 90/566]  eta: 1:03:25  lr: 0.000810  loss: 0.3389 (1.2380)  loss_classifier: 0.2034 (1.0930)  loss_box_reg: 0.1291 (0.1379)  loss_objectness: 0.0007 (0.0019)  loss_rpn_box_reg: 0.0046 (0.0052)  time: 7.9920  data: 0.0337\n",
      "Epoch: [0]  [100/566]  eta: 1:01:54  lr: 0.000898  loss: 0.3223 (1.1472)  loss_classifier: 0.1907 (1.0037)  loss_box_reg: 0.1253 (0.1364)  loss_objectness: 0.0007 (0.0018)  loss_rpn_box_reg: 0.0049 (0.0053)  time: 7.7483  data: 0.0311\n",
      "Epoch: [0]  [110/566]  eta: 1:00:41  lr: 0.000986  loss: 0.3223 (1.0759)  loss_classifier: 0.1907 (0.9324)  loss_box_reg: 0.1262 (0.1364)  loss_objectness: 0.0007 (0.0018)  loss_rpn_box_reg: 0.0053 (0.0053)  time: 7.9362  data: 0.0313\n",
      "Epoch: [0]  [120/566]  eta: 0:59:28  lr: 0.001075  loss: 0.3437 (1.0148)  loss_classifier: 0.2034 (0.8718)  loss_box_reg: 0.1343 (0.1360)  loss_objectness: 0.0003 (0.0017)  loss_rpn_box_reg: 0.0050 (0.0052)  time: 8.1541  data: 0.0325\n",
      "Epoch: [0]  [130/566]  eta: 0:58:17  lr: 0.001163  loss: 0.3384 (0.9632)  loss_classifier: 0.1996 (0.8205)  loss_box_reg: 0.1322 (0.1359)  loss_objectness: 0.0002 (0.0016)  loss_rpn_box_reg: 0.0049 (0.0052)  time: 8.2315  data: 0.0336\n",
      "Epoch: [0]  [140/566]  eta: 0:57:15  lr: 0.001252  loss: 0.3253 (0.9176)  loss_classifier: 0.1911 (0.7757)  loss_box_reg: 0.1294 (0.1352)  loss_objectness: 0.0002 (0.0016)  loss_rpn_box_reg: 0.0051 (0.0052)  time: 8.4448  data: 0.0337\n",
      "Epoch: [0]  [150/566]  eta: 0:56:25  lr: 0.001340  loss: 0.3244 (0.8796)  loss_classifier: 0.1917 (0.7377)  loss_box_reg: 0.1293 (0.1352)  loss_objectness: 0.0002 (0.0015)  loss_rpn_box_reg: 0.0049 (0.0052)  time: 8.9056  data: 0.0370\n",
      "Epoch: [0]  [160/566]  eta: 0:55:24  lr: 0.001428  loss: 0.3413 (0.8476)  loss_classifier: 0.2009 (0.7052)  loss_box_reg: 0.1379 (0.1358)  loss_objectness: 0.0002 (0.0015)  loss_rpn_box_reg: 0.0048 (0.0052)  time: 9.0761  data: 0.0401\n",
      "Epoch: [0]  [170/566]  eta: 0:54:35  lr: 0.001517  loss: 0.3544 (0.8187)  loss_classifier: 0.2116 (0.6762)  loss_box_reg: 0.1395 (0.1360)  loss_objectness: 0.0002 (0.0014)  loss_rpn_box_reg: 0.0048 (0.0052)  time: 9.2688  data: 0.0367\n",
      "Epoch: [0]  [180/566]  eta: 0:53:38  lr: 0.001605  loss: 0.3465 (0.7930)  loss_classifier: 0.2040 (0.6502)  loss_box_reg: 0.1399 (0.1362)  loss_objectness: 0.0001 (0.0014)  loss_rpn_box_reg: 0.0048 (0.0052)  time: 9.5386  data: 0.0336\n",
      "Epoch: [0]  [190/566]  eta: 0:52:27  lr: 0.001694  loss: 0.3465 (0.7696)  loss_classifier: 0.2014 (0.6268)  loss_box_reg: 0.1399 (0.1363)  loss_objectness: 0.0001 (0.0013)  loss_rpn_box_reg: 0.0048 (0.0052)  time: 9.2168  data: 0.0348\n",
      "Epoch: [0]  [200/566]  eta: 0:50:55  lr: 0.001782  loss: 0.3505 (0.7492)  loss_classifier: 0.2030 (0.6061)  loss_box_reg: 0.1400 (0.1366)  loss_objectness: 0.0001 (0.0013)  loss_rpn_box_reg: 0.0050 (0.0052)  time: 8.4281  data: 0.0334\n",
      "Epoch: [0]  [210/566]  eta: 0:49:20  lr: 0.001870  loss: 0.3595 (0.7308)  loss_classifier: 0.2108 (0.5875)  loss_box_reg: 0.1429 (0.1369)  loss_objectness: 0.0001 (0.0012)  loss_rpn_box_reg: 0.0058 (0.0052)  time: 7.8083  data: 0.0312\n",
      "Epoch: [0]  [220/566]  eta: 0:47:48  lr: 0.001959  loss: 0.3698 (0.7147)  loss_classifier: 0.2160 (0.5707)  loss_box_reg: 0.1464 (0.1375)  loss_objectness: 0.0001 (0.0012)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 7.7326  data: 0.0309\n",
      "Epoch: [0]  [230/566]  eta: 0:46:20  lr: 0.002047  loss: 0.3732 (0.7003)  loss_classifier: 0.2166 (0.5557)  loss_box_reg: 0.1517 (0.1381)  loss_objectness: 0.0000 (0.0012)  loss_rpn_box_reg: 0.0061 (0.0053)  time: 7.8212  data: 0.0312\n",
      "Epoch: [0]  [240/566]  eta: 0:44:52  lr: 0.002136  loss: 0.4062 (0.6886)  loss_classifier: 0.2366 (0.5427)  loss_box_reg: 0.1614 (0.1393)  loss_objectness: 0.0000 (0.0012)  loss_rpn_box_reg: 0.0069 (0.0054)  time: 7.9032  data: 0.0317\n",
      "Epoch: [0]  [250/566]  eta: 0:43:24  lr: 0.002224  loss: 0.4098 (0.6779)  loss_classifier: 0.2366 (0.5308)  loss_box_reg: 0.1650 (0.1405)  loss_objectness: 0.0000 (0.0011)  loss_rpn_box_reg: 0.0070 (0.0055)  time: 7.8728  data: 0.0315\n",
      "Epoch: [0]  [260/566]  eta: 0:41:56  lr: 0.002312  loss: 0.4080 (0.6675)  loss_classifier: 0.2346 (0.5195)  loss_box_reg: 0.1651 (0.1414)  loss_objectness: 0.0000 (0.0011)  loss_rpn_box_reg: 0.0075 (0.0056)  time: 7.7921  data: 0.0317\n",
      "Epoch: [0]  [270/566]  eta: 0:40:29  lr: 0.002401  loss: 0.4221 (0.6598)  loss_classifier: 0.2435 (0.5101)  loss_box_reg: 0.1730 (0.1430)  loss_objectness: 0.0000 (0.0011)  loss_rpn_box_reg: 0.0071 (0.0056)  time: 7.7656  data: 0.0316\n",
      "Epoch: [0]  [280/566]  eta: 0:39:02  lr: 0.002489  loss: 0.4694 (0.6530)  loss_classifier: 0.2688 (0.5016)  loss_box_reg: 0.1886 (0.1446)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0075 (0.0057)  time: 7.7704  data: 0.0317\n",
      "Epoch: [0]  [290/566]  eta: 0:37:39  lr: 0.002578  loss: 0.4675 (0.6463)  loss_classifier: 0.2688 (0.4936)  loss_box_reg: 0.1886 (0.1459)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0080 (0.0058)  time: 7.8929  data: 0.0320\n",
      "Epoch: [0]  [300/566]  eta: 0:36:14  lr: 0.002666  loss: 0.4578 (0.6401)  loss_classifier: 0.2652 (0.4861)  loss_box_reg: 0.1817 (0.1472)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0083 (0.0059)  time: 7.9341  data: 0.0316\n",
      "Epoch: [0]  [310/566]  eta: 0:34:50  lr: 0.002754  loss: 0.4443 (0.6341)  loss_classifier: 0.2599 (0.4789)  loss_box_reg: 0.1784 (0.1482)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0089 (0.0060)  time: 7.8957  data: 0.0315\n",
      "Epoch: [0]  [320/566]  eta: 0:33:25  lr: 0.002843  loss: 0.4510 (0.6287)  loss_classifier: 0.2574 (0.4723)  loss_box_reg: 0.1785 (0.1493)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0099 (0.0062)  time: 7.8205  data: 0.0317\n",
      "Epoch: [0]  [330/566]  eta: 0:32:01  lr: 0.002931  loss: 0.4631 (0.6233)  loss_classifier: 0.2730 (0.4660)  loss_box_reg: 0.1785 (0.1499)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0116 (0.0064)  time: 7.7237  data: 0.0315\n",
      "Epoch: [0]  [340/566]  eta: 0:30:37  lr: 0.003020  loss: 0.4608 (0.6185)  loss_classifier: 0.2707 (0.4603)  loss_box_reg: 0.1799 (0.1508)  loss_objectness: 0.0000 (0.0010)  loss_rpn_box_reg: 0.0116 (0.0066)  time: 7.7743  data: 0.0313\n",
      "Epoch: [0]  [350/566]  eta: 0:29:16  lr: 0.003108  loss: 0.4665 (0.6140)  loss_classifier: 0.2726 (0.4548)  loss_box_reg: 0.1799 (0.1515)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0117 (0.0067)  time: 7.9917  data: 0.0318\n",
      "Epoch: [0]  [360/566]  eta: 0:27:54  lr: 0.003196  loss: 0.4633 (0.6103)  loss_classifier: 0.2739 (0.4501)  loss_box_reg: 0.1767 (0.1522)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0131 (0.0070)  time: 8.0956  data: 0.0323\n",
      "Epoch: [0]  [370/566]  eta: 0:26:32  lr: 0.003285  loss: 0.4443 (0.6058)  loss_classifier: 0.2621 (0.4450)  loss_box_reg: 0.1643 (0.1526)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0150 (0.0072)  time: 8.0506  data: 0.0320\n",
      "Epoch: [0]  [380/566]  eta: 0:25:09  lr: 0.003373  loss: 0.4404 (0.6016)  loss_classifier: 0.2612 (0.4404)  loss_box_reg: 0.1624 (0.1529)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0150 (0.0074)  time: 7.9206  data: 0.0331\n",
      "Epoch: [0]  [390/566]  eta: 0:23:48  lr: 0.003462  loss: 0.4404 (0.5977)  loss_classifier: 0.2668 (0.4360)  loss_box_reg: 0.1577 (0.1531)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0150 (0.0077)  time: 7.8694  data: 0.0333\n",
      "Epoch: [0]  [400/566]  eta: 0:22:26  lr: 0.003550  loss: 0.4357 (0.5937)  loss_classifier: 0.2619 (0.4316)  loss_box_reg: 0.1563 (0.1532)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0184 (0.0080)  time: 7.9755  data: 0.0313\n",
      "Epoch: [0]  [410/566]  eta: 0:21:03  lr: 0.003639  loss: 0.4308 (0.5896)  loss_classifier: 0.2619 (0.4274)  loss_box_reg: 0.1523 (0.1531)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0174 (0.0082)  time: 7.8605  data: 0.0309\n",
      "Epoch: [0]  [420/566]  eta: 0:19:41  lr: 0.003727  loss: 0.4135 (0.5857)  loss_classifier: 0.2530 (0.4233)  loss_box_reg: 0.1515 (0.1530)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0169 (0.0085)  time: 7.7983  data: 0.0311\n",
      "Epoch: [0]  [430/566]  eta: 0:18:20  lr: 0.003815  loss: 0.4322 (0.5821)  loss_classifier: 0.2622 (0.4196)  loss_box_reg: 0.1467 (0.1530)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0190 (0.0087)  time: 7.8415  data: 0.0312\n",
      "Epoch: [0]  [440/566]  eta: 0:16:58  lr: 0.003904  loss: 0.4210 (0.5784)  loss_classifier: 0.2564 (0.4158)  loss_box_reg: 0.1445 (0.1527)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0201 (0.0090)  time: 7.7926  data: 0.0313\n",
      "Epoch: [0]  [450/566]  eta: 0:15:36  lr: 0.003992  loss: 0.4131 (0.5750)  loss_classifier: 0.2504 (0.4123)  loss_box_reg: 0.1435 (0.1526)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0211 (0.0093)  time: 7.7828  data: 0.0308\n",
      "Epoch: [0]  [460/566]  eta: 0:14:15  lr: 0.004081  loss: 0.4250 (0.5717)  loss_classifier: 0.2570 (0.4088)  loss_box_reg: 0.1407 (0.1524)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0228 (0.0096)  time: 7.7702  data: 0.0302\n",
      "Epoch: [0]  [470/566]  eta: 0:12:54  lr: 0.004169  loss: 0.4250 (0.5687)  loss_classifier: 0.2498 (0.4056)  loss_box_reg: 0.1433 (0.1523)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0245 (0.0100)  time: 7.7956  data: 0.0305\n",
      "Epoch: [0]  [480/566]  eta: 0:11:33  lr: 0.004257  loss: 0.4212 (0.5658)  loss_classifier: 0.2489 (0.4025)  loss_box_reg: 0.1433 (0.1521)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0270 (0.0104)  time: 8.0521  data: 0.0314\n",
      "Epoch: [0]  [490/566]  eta: 0:10:14  lr: 0.004346  loss: 0.4212 (0.5633)  loss_classifier: 0.2497 (0.3997)  loss_box_reg: 0.1437 (0.1521)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0265 (0.0108)  time: 8.6159  data: 0.0313\n",
      "Epoch: [0]  [500/566]  eta: 0:08:56  lr: 0.004434  loss: 0.4311 (0.5607)  loss_classifier: 0.2573 (0.3968)  loss_box_reg: 0.1435 (0.1519)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0299 (0.0112)  time: 9.4710  data: 0.0312\n",
      "Epoch: [0]  [510/566]  eta: 0:07:39  lr: 0.004523  loss: 0.4465 (0.5597)  loss_classifier: 0.2739 (0.3950)  loss_box_reg: 0.1512 (0.1523)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0318 (0.0116)  time: 11.1193  data: 0.0328\n",
      "Epoch: [0]  [520/566]  eta: 0:06:22  lr: 0.004611  loss: 0.5047 (0.5587)  loss_classifier: 0.3082 (0.3933)  loss_box_reg: 0.1700 (0.1527)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0296 (0.0119)  time: 12.8486  data: 0.0322\n",
      "Epoch: [0]  [530/566]  eta: 0:05:03  lr: 0.004699  loss: 0.4917 (0.5577)  loss_classifier: 0.2988 (0.3916)  loss_box_reg: 0.1659 (0.1530)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0296 (0.0123)  time: 14.2907  data: 0.0325\n",
      "Epoch: [0]  [540/566]  eta: 0:03:41  lr: 0.004788  loss: 0.4917 (0.5569)  loss_classifier: 0.2961 (0.3900)  loss_box_reg: 0.1659 (0.1533)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0334 (0.0128)  time: 14.5893  data: 0.0346\n",
      "Epoch: [0]  [550/566]  eta: 0:02:17  lr: 0.004876  loss: 0.5107 (0.5563)  loss_classifier: 0.2977 (0.3886)  loss_box_reg: 0.1648 (0.1536)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0401 (0.0133)  time: 13.4499  data: 0.0340\n",
      "Epoch: [0]  [560/566]  eta: 0:00:52  lr: 0.004965  loss: 0.5112 (0.5557)  loss_classifier: 0.3036 (0.3871)  loss_box_reg: 0.1708 (0.1540)  loss_objectness: 0.0000 (0.0008)  loss_rpn_box_reg: 0.0401 (0.0138)  time: 12.5426  data: 0.0334\n",
      "Epoch: [0]  [565/566]  eta: 0:00:08  lr: 0.005000  loss: 0.5112 (0.5553)  loss_classifier: 0.3041 (0.3863)  loss_box_reg: 0.1708 (0.1540)  loss_objectness: 0.0000 (0.0009)  loss_rpn_box_reg: 0.0401 (0.0141)  time: 13.8248  data: 0.0336\n",
      "Epoch: [0] Total time: 1:22:44 (8.7716 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:01:55  model_time: 28.7881 (28.7881)  evaluator_time: 0.0149 (0.0149)  time: 28.8311  data: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████▌                                                                                                                                                             | 1/10 [1:24:12<12:37:48, 5052.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [3/4]  eta: 0:00:18  model_time: 21.1218 (18.9122)  evaluator_time: 0.0099 (0.0093)  time: 18.9449  data: 0.0175\n",
      "Test: Total time: 0:01:15 (18.9450 s / it)\n",
      "Averaged stats: model_time: 21.1218 (18.9122)  evaluator_time: 0.0099 (0.0093)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.112\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.147\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.135\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.112\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.183\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.183\n",
      "Epoch: [1]  [  0/566]  eta: 3:07:28  lr: 0.005000  loss: 0.5458 (0.5458)  loss_classifier: 0.3125 (0.3125)  loss_box_reg: 0.1785 (0.1785)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0547 (0.0547)  time: 19.8734  data: 0.0226\n",
      "Epoch: [1]  [ 10/566]  eta: 3:49:11  lr: 0.005000  loss: 0.5062 (0.5071)  loss_classifier: 0.3031 (0.2987)  loss_box_reg: 0.1607 (0.1623)  loss_objectness: 0.0000 (0.0016)  loss_rpn_box_reg: 0.0418 (0.0446)  time: 24.7321  data: 0.0310\n",
      "Epoch: [1]  [ 20/566]  eta: 4:08:49  lr: 0.005000  loss: 0.4598 (0.4846)  loss_classifier: 0.2703 (0.2840)  loss_box_reg: 0.1440 (0.1529)  loss_objectness: 0.0000 (0.0015)  loss_rpn_box_reg: 0.0446 (0.0462)  time: 27.7171  data: 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████▌                                                                                                                                                             | 1/10 [1:35:01<14:15:09, 5701.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c8/8vq529_s3hn08zv7h2jmypgh0000gn/T/ipykernel_23802/3518001510.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_faster_rcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c8/8vq529_s3hn08zv7h2jmypgh0000gn/T/ipykernel_23802/1265673953.py\u001b[0m in \u001b[0;36mtrain_faster_rcnn\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/dog_breed_id/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_training_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mselect_training_samples\u001b[0;34m(self, proposals, targets)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# get matching gt indices for each proposal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mmatched_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_targets_to_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;31m# sample a fixed proportion of positive-negative proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0msampled_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/dog_breed_id/venv31013/lib/python3.10/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36massign_targets_to_proposals\u001b[0;34m(self, proposals, gt_boxes, gt_labels)\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0mmatched_idxs_in_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_quality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0mclamped_matched_idxs_in_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatched_idxs_in_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mlabels_in_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_labels_in_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclamped_matched_idxs_in_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_faster_rcnn(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98773be-d628-47cc-b736-0ca4c1e504bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DogsSubsetDataset(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cccb6efb-3811-47af-8aee-bdfc60e327c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1373, 0.1255, 0.1255,  ..., 0.1922, 0.1804, 0.4549],\n",
       "         [0.1412, 0.1333, 0.1255,  ..., 0.1765, 0.1608, 0.3725],\n",
       "         [0.1333, 0.1294, 0.1137,  ..., 0.1804, 0.1725, 0.3176],\n",
       "         ...,\n",
       "         [0.8118, 0.7882, 0.7882,  ..., 0.2549, 0.2275, 0.2275],\n",
       "         [0.8157, 0.7333, 0.6471,  ..., 0.2314, 0.2275, 0.2275],\n",
       "         [0.8196, 0.8039, 0.8000,  ..., 0.2431, 0.2157, 0.2157]],\n",
       "\n",
       "        [[0.1216, 0.1216, 0.1294,  ..., 0.2118, 0.2078, 0.4863],\n",
       "         [0.1255, 0.1294, 0.1294,  ..., 0.1961, 0.1882, 0.4039],\n",
       "         [0.1176, 0.1255, 0.1176,  ..., 0.2000, 0.2000, 0.3451],\n",
       "         ...,\n",
       "         [0.6902, 0.6667, 0.6667,  ..., 0.2431, 0.2235, 0.2235],\n",
       "         [0.6941, 0.6118, 0.5255,  ..., 0.2157, 0.2157, 0.2157],\n",
       "         [0.6980, 0.6824, 0.6863,  ..., 0.2196, 0.1922, 0.2000]],\n",
       "\n",
       "        [[0.1098, 0.1059, 0.1098,  ..., 0.2863, 0.2824, 0.5608],\n",
       "         [0.1137, 0.1137, 0.1098,  ..., 0.2706, 0.2627, 0.4784],\n",
       "         [0.1059, 0.1098, 0.0980,  ..., 0.2745, 0.2745, 0.4196],\n",
       "         ...,\n",
       "         [0.5098, 0.4863, 0.4941,  ..., 0.2235, 0.2078, 0.2078],\n",
       "         [0.5137, 0.4314, 0.3529,  ..., 0.1725, 0.1804, 0.1804],\n",
       "         [0.5176, 0.5020, 0.5137,  ..., 0.1647, 0.1373, 0.1529]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to\n",
    "model(ds[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaa15a-9a77-4335-aa41-0fa98d7c956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd42e7-0b8e-4266-b0af-c7f29b5ac140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24230ae4-d7f7-44c7-b4e5-6162f5fdf9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d0a28-bbeb-45ef-a1ee-02907b0be5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb8ec7-6eb9-4aac-a7c2-1619115ed8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75893b7f-6110-4825-84f4-3c3e370adf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72fd2a-606a-4a2c-8ece-efe3ff3b1a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9687d-ea98-4b4d-be41-38d74e406ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b7079-47cc-4d5d-8f92-89b97e009630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9abd6552-5201-41ed-bce8-2bebe6bf9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9f676-9e76-464f-93df-639c17a4a3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
